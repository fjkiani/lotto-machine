#!/usr/bin/env python3
"""
ğŸ”¥ REDDIT EXPLOITER - Phase 5 Exploitation Module
==================================================

Exploits Reddit sentiment for contrarian trading signals.

KEY CAPABILITIES:
1. HOT TICKER DISCOVERY - Find what retail is talking about
2. SENTIMENT MOMENTUM - Track sentiment changes over time
3. CONTRARIAN SIGNALS - Fade the hype, fade the fear
4. STEALTH DETECTION - Find tickers with low noise (smart money)

DATA SOURCE: ChartExchange Reddit API
- 774K+ TSLA mentions available
- Real-time sentiment scoring
- Subreddit breakdown
- Pagination support

Author: Alpha's AI Hedge Fund
Date: 2024-12-16
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass, field
from enum import Enum
from collections import Counter
import requests
import time
import sqlite3
import json
import os

# Price data
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False
    logger.warning("yfinance not available - price correlation disabled")

# Add paths
sys.path.append(str(Path(__file__).parent.parent.parent / 'core/data'))
sys.path.append(str(Path(__file__).parent.parent.parent / 'configs'))

logger = logging.getLogger(__name__)


class RedditSignalType(Enum):
    """Types of Reddit-based signals"""
    FADE_HYPE = "FADE_HYPE"          # Extreme bullish = SHORT signal
    FADE_FEAR = "FADE_FEAR"          # Extreme bearish = LONG signal
    MOMENTUM_SURGE = "MOMENTUM_SURGE" # Rapid mention increase = momentum play
    STEALTH_MOVE = "STEALTH_MOVE"    # Low mentions + price move = smart money
    PUMP_WARNING = "PUMP_WARNING"    # Sudden spike = potential pump & dump
    SENTIMENT_FLIP = "SENTIMENT_FLIP" # Sentiment reversal = trend change
    SENTIMENT_SHIFT_ALERT = "SENTIMENT_SHIFT_ALERT"  # Historical trend shift detected
    VELOCITY_SURGE = "VELOCITY_SURGE"  # Mention velocity > 3x normal (early pump warning)
    BULLISH_DIVERGENCE = "BULLISH_DIVERGENCE"  # Price down, sentiment up (accumulation)
    BEARISH_DIVERGENCE = "BEARISH_DIVERGENCE"  # Price up, sentiment down (distribution)
    STEALTH_ACCUMULATION = "STEALTH_ACCUMULATION"  # Low mentions, price rising quietly


@dataclass
class RedditMention:
    """Single Reddit mention with sentiment"""
    subreddit: str
    created: datetime
    sentiment: float  # -1 to +1
    author: str
    text: str
    link: str
    thing_type: str  # 'comment' or 'submission'


@dataclass
class RedditTickerAnalysis:
    """Complete Reddit analysis for a ticker"""
    symbol: str
    timestamp: datetime
    
    # Mention metrics
    total_mentions: int
    mentions_today: int
    mentions_24h_ago: int
    mention_change_pct: float
    
    # Sentiment metrics
    avg_sentiment: float
    bullish_pct: float  # % of posts with sentiment > 0.3
    bearish_pct: float  # % of posts with sentiment < -0.3
    sentiment_trend: str  # "IMPROVING", "DECLINING", "STABLE"
    
    # Subreddit breakdown
    top_subreddits: Dict[str, int]
    wsb_dominance: float  # % from r/wallstreetbets
    
    # Signal
    signal_type: Optional[RedditSignalType]
    signal_strength: float  # 0-100
    action: str  # "LONG", "SHORT", "AVOID", "WATCH"
    
    # Context
    reasoning: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    sample_posts: List[str] = field(default_factory=list)


@dataclass
class HotTickerDiscovery:
    """Hot ticker discovered from Reddit"""
    symbol: str
    mention_count: int
    avg_sentiment: float
    bullish_pct: float
    wsb_mentions: int
    momentum_score: float  # How fast mentions are growing
    discovery_reason: str


class SentimentHistory:
    """
    Production-grade sentiment history tracking with SQLite persistence.
    
    Tracks sentiment changes over time to catch trends EARLY.
    Persists data across restarts using SQLite database.
    """
    
    def __init__(self, symbol: str, db_path: Optional[str] = None):
        """
        Initialize sentiment history tracker.
        
        Args:
            symbol: Ticker symbol
            db_path: Path to SQLite database (default: data/reddit_sentiment_history.db)
        """
        self.symbol = symbol.upper()
        
        # Database setup
        if db_path is None:
            data_dir = Path(__file__).parent.parent.parent / 'data'
            data_dir.mkdir(exist_ok=True)
            db_path = str(data_dir / 'reddit_sentiment_history.db')
        
        self.db_path = db_path
        self._init_database()
    
    def _init_database(self):
        """Initialize SQLite database and create table if needed."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS sentiment_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                sentiment REAL NOT NULL,
                mention_count INTEGER NOT NULL,
                bullish_pct REAL,
                bearish_pct REAL,
                wsb_dominance REAL,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(symbol, timestamp)
            )
        ''')
        
        # Create index for faster queries
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_symbol_timestamp 
            ON sentiment_history(symbol, timestamp DESC)
        ''')
        
        conn.commit()
        conn.close()
    
    def add_datapoint(self, timestamp: datetime, sentiment: float, mentions: int,
                     bullish_pct: Optional[float] = None, bearish_pct: Optional[float] = None,
                     wsb_dominance: Optional[float] = None):
        """
        Add a sentiment datapoint to history.
        
        Args:
            timestamp: When this sentiment was measured
            sentiment: Average sentiment (-1 to +1)
            mentions: Number of mentions
            bullish_pct: Percentage of bullish posts
            bearish_pct: Percentage of bearish posts
            wsb_dominance: Percentage from WSB
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        timestamp_str = timestamp.strftime('%Y-%m-%d %H:%M:%S')
        
        cursor.execute('''
            INSERT OR REPLACE INTO sentiment_history 
            (symbol, timestamp, sentiment, mention_count, bullish_pct, bearish_pct, wsb_dominance)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (self.symbol, timestamp_str, sentiment, mentions, bullish_pct, bearish_pct, wsb_dominance))
        
        conn.commit()
        conn.close()
    
    def get_history(self, days: int = 7) -> List[Tuple[datetime, float, int]]:
        """
        Get sentiment history for this symbol.
        
        Args:
            days: Number of days to retrieve
        
        Returns:
            List of (timestamp, sentiment, mention_count) tuples
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cutoff = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')
        
        cursor.execute('''
            SELECT timestamp, sentiment, mention_count
            FROM sentiment_history
            WHERE symbol = ? AND timestamp >= ?
            ORDER BY timestamp ASC
        ''', (self.symbol, cutoff))
        
        results = cursor.fetchall()
        conn.close()
        
        history = []
        for row in results:
            timestamp = datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')
            sentiment = row[1]
            mentions = row[2]
            history.append((timestamp, sentiment, mentions))
        
        return history
    
    def get_trend(self, lookback_days: int = 7) -> str:
        """
        Calculate sentiment trend from historical data.
        
        Args:
            lookback_days: Days of history to analyze
        
        Returns:
            "BULLISH_SHIFT", "BEARISH_SHIFT", "STABLE", or "INSUFFICIENT_DATA"
        """
        history = self.get_history(days=lookback_days)
        
        if len(history) < 5:
            return "INSUFFICIENT_DATA"
        
        # Split into recent (last 3) and older (rest)
        recent = history[-3:]
        older = history[:-3]
        
        if not older:
            return "INSUFFICIENT_DATA"
        
        recent_avg = sum(s for _, s, _ in recent) / len(recent)
        older_avg = sum(s for _, s, _ in older) / len(older)
        
        # Threshold: 0.15 change = significant shift
        if recent_avg > older_avg + 0.15:
            return "BULLISH_SHIFT"
        elif recent_avg < older_avg - 0.15:
            return "BEARISH_SHIFT"
        else:
            return "STABLE"
    
    def get_sentiment_momentum(self) -> float:
        """
        Calculate sentiment momentum (rate of change).
        
        Returns:
            Momentum score (-1 to +1, positive = improving)
        """
        history = self.get_history(days=7)
        
        if len(history) < 2:
            return 0.0
        
        # Linear regression slope (simple)
        recent = history[-5:] if len(history) >= 5 else history
        if len(recent) < 2:
            return 0.0
        
        sentiments = [s for _, s, _ in recent]
        first_sentiment = sentiments[0]
        last_sentiment = sentiments[-1]
        
        # Normalize momentum
        momentum = (last_sentiment - first_sentiment) / max(abs(first_sentiment), 0.1)
        return max(-1.0, min(1.0, momentum))
    
    def cleanup_old_data(self, days_to_keep: int = 30):
        """
        Clean up old sentiment data (keep last N days).
        
        Args:
            days_to_keep: Number of days to keep
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cutoff = (datetime.now() - timedelta(days=days_to_keep)).strftime('%Y-%m-%d %H:%M:%S')
        
        cursor.execute('''
            DELETE FROM sentiment_history
            WHERE symbol = ? AND timestamp < ?
        ''', (self.symbol, cutoff))
        
        deleted = cursor.rowcount
        conn.commit()
        conn.close()
        
        if deleted > 0:
            logger.debug(f"   Cleaned up {deleted} old datapoints for {self.symbol}")


class RedditExploiter:
    """
    ğŸ”¥ Reddit Exploitation Engine
    
    Scans Reddit for trading opportunities by:
    1. Finding HOT tickers (what retail is talking about)
    2. Tracking sentiment momentum (bullish/bearish shifts)
    3. Generating contrarian signals (fade hype/fear)
    4. Detecting stealth moves (smart money accumulation)
    """
    
    # Thresholds
    FADE_HYPE_THRESHOLD = 0.4      # Avg sentiment > 0.4 = fade
    FADE_FEAR_THRESHOLD = -0.3    # Avg sentiment < -0.3 = buy fear
    BULLISH_PCT_EXTREME = 60      # > 60% bullish = too crowded
    BEARISH_PCT_EXTREME = 50      # > 50% bearish = fear
    MENTION_SURGE_THRESHOLD = 2.0  # 2x normal = surge
    WSB_DOMINANCE_WARNING = 70    # > 70% from WSB = meme risk
    
    # Signal strength weights
    SENTIMENT_WEIGHT = 40
    MOMENTUM_WEIGHT = 30
    VOLUME_WEIGHT = 20
    CONTRARIAN_WEIGHT = 10
    
    def __init__(self, api_key: str):
        """
        Initialize Reddit Exploiter
        
        Args:
            api_key: ChartExchange API key
        """
        self.api_key = api_key
        self.base_url = "https://chartexchange.com/api/v1"
        self.session = requests.Session()
        
        # Rate limiting (Tier 3: 1000 requests/minute)
        self.rate_limit_per_minute = 1000
        self.request_times = []  # Track request timestamps
        self.request_lock = False  # Lock to prevent concurrent rate limit checks
        
        # Cache for rate limiting
        self.cache = {}
        self.cache_ttl = 300  # 5 minutes
        
        # Priority tiers for tickers (higher = more important)
        self.ticker_priorities = {
            # Tier 1: Always scan (highest priority)
            'TSLA': 10, 'NVDA': 10, 'AAPL': 10, 'MSFT': 10, 'SPY': 10, 'QQQ': 10,
            # Tier 2: High priority
            'GME': 8, 'AMC': 8, 'AMD': 8, 'GOOGL': 8, 'AMZN': 8, 'META': 8,
            # Tier 3: Medium priority
            'PLTR': 6, 'SOFI': 6, 'MSTR': 6, 'COIN': 6, 'RIOT': 6, 'MARA': 6,
            # Tier 4: Lower priority (default)
        }
        
        # Ticker universe for scanning
        self.scan_universe = [
            # Mega caps (always relevant)
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA',
            # Semiconductors (AI plays)
            'AMD', 'INTC', 'AVGO', 'MU', 'SMCI', 'ARM', 'QCOM',
            # Meme favorites
            'GME', 'AMC', 'BB', 'KOSS', 'BBBY',
            # Crypto-related
            'MSTR', 'COIN', 'MARA', 'RIOT', 'HUT',
            # AI/Cloud
            'PLTR', 'AI', 'SNOW', 'DDOG', 'NET', 'CRM',
            # EVs
            'RIVN', 'LCID', 'NIO', 'XPEV', 'LI',
            # Fintech
            'SOFI', 'HOOD', 'AFRM', 'UPST', 'SQ', 'PYPL',
            # Other popular
            'ROKU', 'SHOP', 'ZM', 'DOCU', 'CRWD', 'ZS'
        ]
        
        # Sentiment history tracking (Task 5.2)
        self.sentiment_histories: Dict[str, SentimentHistory] = {}
        self.history_db_path = None  # Will be set on first use
        
        logger.info("ğŸ”¥ Reddit Exploiter initialized")
        logger.info(f"   Rate limit: {self.rate_limit_per_minute} requests/minute")
        logger.info(f"   Cache TTL: {self.cache_ttl} seconds")
        logger.info(f"   Ticker universe: {len(self.scan_universe)} tickers")
        logger.info(f"   Sentiment history: SQLite persistence enabled")
    
    def _check_rate_limit(self) -> bool:
        """
        Check if we can make a request without hitting rate limit.
        
        Returns:
            True if request allowed, False if rate limit would be exceeded
        """
        now = datetime.now()
        
        # Clean old requests (older than 1 minute)
        cutoff = now - timedelta(minutes=1)
        self.request_times = [t for t in self.request_times if t > cutoff]
        
        # Check if we're at limit
        if len(self.request_times) >= self.rate_limit_per_minute:
            logger.warning(f"âš ï¸ Rate limit reached: {len(self.request_times)}/{self.rate_limit_per_minute} requests in last minute")
            return False
        
        return True
    
    def _wait_for_rate_limit(self, max_wait_seconds: int = 5):
        """
        Wait until rate limit allows a request.
        
        Args:
            max_wait_seconds: Maximum seconds to wait
        """
        waited = 0
        while not self._check_rate_limit() and waited < max_wait_seconds:
            time.sleep(0.5)
            waited += 0.5
        
        if waited >= max_wait_seconds:
            logger.warning(f"âš ï¸ Rate limit wait timeout after {max_wait_seconds}s")
    
    def _record_request(self):
        """Record a request timestamp for rate limiting."""
        self.request_times.append(datetime.now())
    
    def _get_ticker_priority(self, symbol: str) -> int:
        """Get priority for a ticker (higher = more important)."""
        return self.ticker_priorities.get(symbol, 5)  # Default priority: 5
    
    def get_rate_limit_status(self) -> Dict:
        """
        Get current rate limit status.
        
        Returns:
            {
                'requests_last_minute': int,
                'rate_limit': int,
                'remaining': int,
                'utilization_pct': float,
                'can_make_request': bool
            }
        """
        now = datetime.now()
        cutoff = now - timedelta(minutes=1)
        recent_requests = [t for t in self.request_times if t > cutoff]
        
        requests_count = len(recent_requests)
        remaining = self.rate_limit_per_minute - requests_count
        utilization = (requests_count / self.rate_limit_per_minute) * 100
        
        return {
            'requests_last_minute': requests_count,
            'rate_limit': self.rate_limit_per_minute,
            'remaining': remaining,
            'utilization_pct': utilization,
            'can_make_request': remaining > 0
        }
    
    def _fetch_mentions(self, symbol: str, days: int = 7, max_pages: int = 3) -> List[RedditMention]:
        """
        Fetch Reddit mentions with pagination support and rate limiting.
        
        Args:
            symbol: Ticker symbol
            days: Number of days to fetch
            max_pages: Maximum pages to fetch (100 per page)
        
        Returns:
            List of RedditMention objects
        """
        cache_key = f"{symbol}_{days}_{max_pages}"
        
        # Check cache first (avoids API call)
        if cache_key in self.cache:
            cached_time, cached_data = self.cache[cache_key]
            if (datetime.now() - cached_time).seconds < self.cache_ttl:
                logger.debug(f"ğŸ“¦ Cache hit for {symbol}")
                return cached_data
        
        # Check rate limit before making request
        if not self._check_rate_limit():
            logger.warning(f"âš ï¸ Rate limit reached, skipping {symbol} (will use cache if available)")
            # Return cached data even if expired (better than nothing)
            if cache_key in self.cache:
                cached_time, cached_data = self.cache[cache_key]
                logger.debug(f"   Using expired cache for {symbol}")
                return cached_data
            return []
        
        mentions = []
        next_url = f"{self.base_url}/data/reddit/mentions/stock/{symbol.upper()}/"
        params = {
            'api_key': self.api_key,
            'days': days
        }
        
        pages_fetched = 0
        
        while next_url and pages_fetched < max_pages:
            # Check rate limit before each page
            if not self._check_rate_limit():
                logger.warning(f"âš ï¸ Rate limit reached while fetching {symbol} (got {pages_fetched} pages)")
                break
            
            try:
                if pages_fetched == 0:
                    resp = self.session.get(next_url, params=params, timeout=10)
                else:
                    # Next URL already has params
                    resp = self.session.get(next_url, timeout=10)
                
                # Record request
                self._record_request()
                
                if resp.status_code == 429:  # Rate limit exceeded
                    logger.warning(f"âš ï¸ API returned 429 (rate limit) for {symbol}")
                    self._wait_for_rate_limit(max_wait_seconds=5)
                    continue
                
                if resp.status_code != 200:
                    logger.warning(f"Reddit API error for {symbol}: {resp.status_code}")
                    break
                
                data = resp.json()
                
                # Handle paginated response
                if isinstance(data, dict):
                    results = data.get('results', [])
                    next_url = data.get('next')
                else:
                    results = data
                    next_url = None
                
                for item in results:
                    try:
                        created = datetime.strptime(item['created'], '%Y-%m-%d %H:%M:%S')
                        mention = RedditMention(
                            subreddit=item.get('subreddit', 'unknown'),
                            created=created,
                            sentiment=float(item.get('sentiment', 0)),
                            author=item.get('author', 'unknown'),
                            text=item.get('text', '')[:500],  # Truncate
                            link=item.get('link', ''),
                            thing_type=item.get('thing_type', 'comment')
                        )
                        mentions.append(mention)
                    except Exception as e:
                        logger.debug(f"Error parsing mention: {e}")
                
                pages_fetched += 1
                
            except Exception as e:
                logger.error(f"Error fetching Reddit mentions for {symbol}: {e}")
                break
        
        # Update cache
        self.cache[cache_key] = (datetime.now(), mentions)
        
        return mentions
    
    def analyze_ticker(self, symbol: str, days: int = 3) -> Optional[RedditTickerAnalysis]:
        """
        Comprehensive Reddit analysis for a single ticker
        
        Args:
            symbol: Ticker symbol
            days: Days to analyze
        
        Returns:
            RedditTickerAnalysis or None
        """
        try:
            logger.info(f"ğŸ“± Analyzing Reddit sentiment for {symbol}...")
            
            # Fetch mentions (up to 300 posts)
            mentions = self._fetch_mentions(symbol, days=days, max_pages=3)
            
            if not mentions:
                logger.warning(f"No Reddit data for {symbol}")
                return None
            
            # Calculate metrics
            total_mentions = len(mentions)
            
            # Split by time
            now = datetime.now()
            today = now.replace(hour=0, minute=0, second=0, microsecond=0)
            yesterday = today - timedelta(days=1)
            
            mentions_today = len([m for m in mentions if m.created >= today])
            mentions_yesterday = len([m for m in mentions if yesterday <= m.created < today])
            
            # Mention change
            if mentions_yesterday > 0:
                mention_change_pct = ((mentions_today - mentions_yesterday) / mentions_yesterday) * 100
            else:
                mention_change_pct = 100.0 if mentions_today > 0 else 0.0
            
            # Sentiment analysis
            sentiments = [m.sentiment for m in mentions]
            avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
            
            bullish_count = len([s for s in sentiments if s > 0.3])
            bearish_count = len([s for s in sentiments if s < -0.3])
            
            bullish_pct = (bullish_count / total_mentions) * 100
            bearish_pct = (bearish_count / total_mentions) * 100
            
            # Sentiment trend (compare first half vs second half)
            mid = len(sentiments) // 2
            if mid > 0:
                first_half_avg = sum(sentiments[:mid]) / mid
                second_half_avg = sum(sentiments[mid:]) / (len(sentiments) - mid)
                
                if second_half_avg > first_half_avg + 0.1:
                    sentiment_trend = "IMPROVING"
                elif second_half_avg < first_half_avg - 0.1:
                    sentiment_trend = "DECLINING"
                else:
                    sentiment_trend = "STABLE"
            else:
                sentiment_trend = "STABLE"
            
            # Subreddit breakdown
            subreddit_counts = Counter([m.subreddit for m in mentions])
            top_subreddits = dict(subreddit_counts.most_common(5))
            
            wsb_count = subreddit_counts.get('wallstreetbets', 0)
            wsb_dominance = (wsb_count / total_mentions) * 100 if total_mentions > 0 else 0
            
            # Track sentiment history (Task 5.2)
            history = self._get_sentiment_history(symbol)
            sentiment_shift_detected = False
            sentiment_shift_reasoning = []
            
            if history:
                history.add_datapoint(
                    timestamp=now,
                    sentiment=avg_sentiment,
                    mentions=total_mentions,
                    bullish_pct=bullish_pct,
                    bearish_pct=bearish_pct,
                    wsb_dominance=wsb_dominance
                )
                
                # Check for sentiment shift
                trend = history.get_trend(lookback_days=7)
                if trend in ["BULLISH_SHIFT", "BEARISH_SHIFT"]:
                    sentiment_shift_detected = True
                    if trend == "BULLISH_SHIFT":
                        sentiment_shift_reasoning.append(f"ğŸ“ˆ Sentiment shift detected: {trend}")
                        sentiment_shift_reasoning.append(f"Historical trend: Sentiment improving over 7 days")
                    elif trend == "BEARISH_SHIFT":
                        sentiment_shift_reasoning.append(f"ğŸ“‰ Sentiment shift detected: {trend}")
                        sentiment_shift_reasoning.append(f"Historical trend: Sentiment declining over 7 days")
            
            # Calculate mention velocity (Task 5.3)
            velocity_data = self.calculate_mention_velocity(symbol)
            velocity_surge_detected = velocity_data.get('is_surging', False)
            velocity_reasoning = []
            
            if velocity_surge_detected:
                surge_multiplier = velocity_data.get('surge_multiplier', 1.0)
                velocity_reasoning.append(f"ğŸš¨ VELOCITY SURGE: {surge_multiplier:.1f}x normal mention rate")
                velocity_reasoning.append(f"Last hour: {velocity_data['velocity_1h']:.1f} mentions/hr")
                velocity_reasoning.append(f"24h avg: {velocity_data['velocity_24h']:.1f} mentions/hr")
                velocity_reasoning.append("âš ï¸ Early pump warning - monitor closely")
            
            # Generate signal
            signal_type, signal_strength, action, reasoning, warnings = self._generate_signal(
                avg_sentiment=avg_sentiment,
                bullish_pct=bullish_pct,
                bearish_pct=bearish_pct,
                mention_change_pct=mention_change_pct,
                wsb_dominance=wsb_dominance,
                sentiment_trend=sentiment_trend,
                velocity_surge=velocity_surge_detected,
                velocity_data=velocity_data
            )
            
            # Add velocity surge signal if detected
            if velocity_surge_detected and signal_type != RedditSignalType.PUMP_WARNING:
                # Velocity surge takes precedence over sentiment shift but not over PUMP_WARNING
                if signal_type is None or signal_strength < 75:
                    signal_type = RedditSignalType.VELOCITY_SURGE
                    action = "AVOID"  # Velocity surge = potential pump, avoid
                    signal_strength = 75
                    reasoning.extend(velocity_reasoning)
                    warnings.append("âš ï¸ High mention velocity - potential manipulation")
            
            # Override with sentiment shift signal if detected and no other strong signal
            if sentiment_shift_detected and (signal_type is None or signal_strength < 70):
                signal_type = RedditSignalType.SENTIMENT_SHIFT_ALERT
                if trend == "BULLISH_SHIFT":
                    action = "WATCH_LONG"
                else:
                    action = "WATCH_SHORT"
                signal_strength = 65
                reasoning.extend(sentiment_shift_reasoning)
            
            # Create preliminary analysis for price correlation
            preliminary_analysis = RedditTickerAnalysis(
                symbol=symbol,
                timestamp=now,
                total_mentions=total_mentions,
                mentions_today=mentions_today,
                mentions_24h_ago=mentions_yesterday,
                mention_change_pct=mention_change_pct,
                avg_sentiment=avg_sentiment,
                bullish_pct=bullish_pct,
                bearish_pct=bearish_pct,
                sentiment_trend=sentiment_trend,
                top_subreddits=top_subreddits,
                wsb_dominance=wsb_dominance,
                signal_type=signal_type,
                signal_strength=signal_strength,
                action=action,
                reasoning=reasoning.copy(),
                warnings=warnings.copy(),
                sample_posts=[]
            )
            
            # Price correlation check (Task 5.7)
            price_correlation = self.correlate_with_price(symbol, preliminary_analysis)
            price_reasoning = []
            
            if price_correlation['divergence']:
                div_type = price_correlation['divergence_type']
                price_change_7d = price_correlation['price_change_7d']
                current_price = price_correlation['current_price']
                
                if div_type == 'BULLISH_DIV':
                    # Price down but sentiment up = accumulation opportunity
                    if signal_type is None or signal_strength < 70:
                        signal_type = RedditSignalType.BULLISH_DIVERGENCE
                        action = "LONG"
                        signal_strength = 70
                        price_reasoning.append(f"ğŸ“ˆ BULLISH DIVERGENCE detected")
                        price_reasoning.append(f"Price: ${current_price:.2f} ({price_change_7d:+.1f}% 7d)")
                        price_reasoning.append(f"Sentiment: {avg_sentiment:+.2f} (improving)")
                        price_reasoning.append("Smart money accumulating - potential reversal")
                
                elif div_type == 'BEARISH_DIV':
                    # Price up but sentiment down = distribution warning
                    if signal_type is None or signal_strength < 70:
                        signal_type = RedditSignalType.BEARISH_DIVERGENCE
                        action = "SHORT"
                        signal_strength = 70
                        price_reasoning.append(f"ğŸ“‰ BEARISH DIVERGENCE detected")
                        price_reasoning.append(f"Price: ${current_price:.2f} ({price_change_7d:+.1f}% 7d)")
                        price_reasoning.append(f"Sentiment: {avg_sentiment:+.2f} (declining)")
                        price_reasoning.append("Smart money distributing - potential top")
            
            # STEALTH_ACCUMULATION: Low mentions but price rising
            if (price_correlation['price_change_7d'] > 5.0 and 
                total_mentions < 50 and 
                avg_sentiment > 0.1 and
                (signal_type is None or signal_strength < 65)):
                signal_type = RedditSignalType.STEALTH_ACCUMULATION
                action = "LONG"
                signal_strength = 65
                price_reasoning.append(f"ğŸ¤« STEALTH ACCUMULATION detected")
                price_reasoning.append(f"Price: ${price_correlation['current_price']:.2f} ({price_correlation['price_change_7d']:+.1f}% 7d)")
                price_reasoning.append(f"Mentions: {total_mentions} (low noise)")
                price_reasoning.append("Smart money buying quietly - follow the institutions")
            
            # Add price correlation info to reasoning
            if price_correlation['current_price'] > 0:
                reasoning.append(f"Price: ${price_correlation['current_price']:.2f} ({price_correlation['price_change_7d']:+.1f}% 7d)")
                reasoning.append(f"Correlation: {price_correlation['confirmation']}")
                if price_reasoning:
                    reasoning.extend(price_reasoning)
            
            # Sample posts
            sample_posts = [
                f"[{m.sentiment:+.2f}] r/{m.subreddit}: {m.text[:100]}..."
                for m in sorted(mentions, key=lambda x: abs(x.sentiment), reverse=True)[:3]
            ]
            
            analysis = RedditTickerAnalysis(
                symbol=symbol,
                timestamp=datetime.now(),
                total_mentions=total_mentions,
                mentions_today=mentions_today,
                mentions_24h_ago=mentions_yesterday,
                mention_change_pct=mention_change_pct,
                avg_sentiment=avg_sentiment,
                bullish_pct=bullish_pct,
                bearish_pct=bearish_pct,
                sentiment_trend=sentiment_trend,
                top_subreddits=top_subreddits,
                wsb_dominance=wsb_dominance,
                signal_type=signal_type,
                signal_strength=signal_strength,
                action=action,
                reasoning=reasoning,
                warnings=warnings,
                sample_posts=sample_posts
            )
            
            logger.info(f"âœ… Reddit analysis complete for {symbol}: {action} ({signal_strength:.0f}%)")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing {symbol}: {e}")
            return None
    
    def _get_sentiment_history(self, symbol: str) -> Optional[SentimentHistory]:
        """
        Get or create sentiment history tracker for a symbol.
        
        Args:
            symbol: Ticker symbol
        
        Returns:
            SentimentHistory instance or None if error
        """
        symbol_upper = symbol.upper()
        
        if symbol_upper not in self.sentiment_histories:
            try:
                # Initialize database path on first use
                if self.history_db_path is None:
                    data_dir = Path(__file__).parent.parent.parent / 'data'
                    data_dir.mkdir(exist_ok=True)
                    self.history_db_path = str(data_dir / 'reddit_sentiment_history.db')
                
                self.sentiment_histories[symbol_upper] = SentimentHistory(
                    symbol=symbol_upper,
                    db_path=self.history_db_path
                )
            except Exception as e:
                logger.warning(f"Failed to create sentiment history for {symbol}: {e}")
                return None
        
        return self.sentiment_histories.get(symbol_upper)
    
    def track_sentiment_history(self, symbols: List[str]) -> Dict[str, SentimentHistory]:
        """
        Track sentiment history for multiple symbols.
        
        Args:
            symbols: List of ticker symbols
        
        Returns:
            Dictionary mapping symbol -> SentimentHistory
        """
        histories = {}
        
        for symbol in symbols:
            history = self._get_sentiment_history(symbol)
            if history:
                histories[symbol] = history
        
        return histories
    
    def cleanup_old_sentiment_data(self, days_to_keep: int = 30):
        """
        Clean up old sentiment data for all tracked symbols.
        
        Args:
            days_to_keep: Number of days to keep
        """
        logger.info(f"ğŸ§¹ Cleaning up sentiment history (keeping last {days_to_keep} days)...")
        
        for symbol, history in self.sentiment_histories.items():
            try:
                history.cleanup_old_data(days_to_keep)
            except Exception as e:
                logger.warning(f"Failed to cleanup {symbol}: {e}")
    
    def calculate_mention_velocity(self, symbol: str) -> Dict:
        """
        Calculate mention velocity (rate of change) - Task 5.3.
        
        Uses sentiment history to track mention rate over time.
        
        Args:
            symbol: Ticker symbol
        
        Returns:
            {
                'velocity_1h': float,      # Mentions per hour (last hour)
                'velocity_24h': float,     # Mentions per hour (last 24h avg)
                'acceleration': float,      # Velocity change rate (1h vs 24h)
                'is_surging': bool,        # True if velocity > 3x normal
                'surge_multiplier': float,  # How many times faster than normal
            }
        """
        history = self._get_sentiment_history(symbol)
        
        if not history:
            return {
                'velocity_1h': 0.0,
                'velocity_24h': 0.0,
                'acceleration': 0.0,
                'is_surging': False,
                'surge_multiplier': 1.0
            }
        
        # Get last 24 hours of history
        now = datetime.now()
        cutoff_24h = now - timedelta(hours=24)
        cutoff_1h = now - timedelta(hours=1)
        
        # Fetch mentions for velocity calculation
        mentions_24h = self._fetch_mentions(symbol, days=1, max_pages=1)
        
        if not mentions_24h:
            return {
                'velocity_1h': 0.0,
                'velocity_24h': 0.0,
                'acceleration': 0.0,
                'is_surging': False,
                'surge_multiplier': 1.0
            }
        
        # Calculate velocity_1h (mentions in last hour)
        mentions_1h = [m for m in mentions_24h if m.created >= cutoff_1h]
        velocity_1h = len(mentions_1h) / 1.0  # Mentions per hour
        
        # Calculate velocity_24h (average mentions per hour over 24h)
        velocity_24h = len(mentions_24h) / 24.0  # Mentions per hour
        
        # Calculate acceleration (rate of change)
        if velocity_24h > 0:
            acceleration = (velocity_1h - velocity_24h) / velocity_24h  # Fractional change
        else:
            acceleration = 0.0
        
        # Determine if surging (velocity > 3x normal)
        surge_threshold = 3.0
        is_surging = velocity_1h > (velocity_24h * surge_threshold) and velocity_24h > 0
        surge_multiplier = velocity_1h / velocity_24h if velocity_24h > 0 else 1.0
        
        return {
            'velocity_1h': velocity_1h,
            'velocity_24h': velocity_24h,
            'acceleration': acceleration,
            'is_surging': is_surging,
            'surge_multiplier': surge_multiplier
        }
    
    def correlate_with_price(self, symbol: str, analysis: RedditTickerAnalysis) -> Dict:
        """
        Correlate Reddit sentiment with price action - Task 5.7.
        
        Args:
            symbol: Ticker symbol
            analysis: RedditTickerAnalysis object
        
        Returns:
            {
                'price_change_24h': float,
                'price_change_7d': float,
                'sentiment_price_correlation': float,  # -1 to +1
                'divergence': bool,  # True if sentiment and price disagree
                'divergence_type': str,  # 'BULLISH_DIV' or 'BEARISH_DIV'
                'confirmation': str,    # 'CONFIRMED', 'DIVERGENT', 'NEUTRAL'
                'current_price': float,
                'price_24h_ago': float,
                'price_7d_ago': float,
            }
        
        Divergence Examples:
        - Price falling but sentiment rising = BULLISH_DIV (potential reversal)
        - Price rising but sentiment falling = BEARISH_DIV (potential top)
        """
        if not YFINANCE_AVAILABLE:
            return {
                'price_change_24h': 0.0,
                'price_change_7d': 0.0,
                'sentiment_price_correlation': 0.0,
                'divergence': False,
                'divergence_type': None,
                'confirmation': 'NEUTRAL',
                'current_price': 0.0,
                'price_24h_ago': 0.0,
                'price_7d_ago': 0.0,
            }
        
        try:
            ticker = yf.Ticker(symbol)
            
            # Get price data
            hist_7d = ticker.history(period='7d', interval='1d')
            if hist_7d.empty:
                logger.warning(f"No price data for {symbol}")
                return {
                    'price_change_24h': 0.0,
                    'price_change_7d': 0.0,
                    'sentiment_price_correlation': 0.0,
                    'divergence': False,
                    'divergence_type': None,
                    'confirmation': 'NEUTRAL',
                    'current_price': 0.0,
                    'price_24h_ago': 0.0,
                    'price_7d_ago': 0.0,
                }
            
            current_price = float(hist_7d['Close'].iloc[-1])
            price_7d_ago = float(hist_7d['Close'].iloc[0])
            
            # Get 24h price (use intraday if available, else estimate from daily)
            hist_1d = ticker.history(period='2d', interval='1h')
            if not hist_1d.empty and len(hist_1d) >= 24:
                # Use actual 24h ago price
                price_24h_ago = float(hist_1d['Close'].iloc[-24])
            else:
                # Estimate from daily data
                if len(hist_7d) >= 2:
                    price_24h_ago = float(hist_7d['Close'].iloc[-2])
                else:
                    price_24h_ago = price_7d_ago
            
            # Calculate price changes
            price_change_24h = ((current_price - price_24h_ago) / price_24h_ago) * 100 if price_24h_ago > 0 else 0.0
            price_change_7d = ((current_price - price_7d_ago) / price_7d_ago) * 100 if price_7d_ago > 0 else 0.0
            
            # Calculate sentiment-price correlation
            # Positive correlation: sentiment up + price up, or sentiment down + price down
            # Negative correlation: sentiment up + price down, or sentiment down + price up
            sentiment_change = analysis.avg_sentiment  # Current sentiment (0 = neutral)
            
            # Normalize sentiment to -1 to +1 scale for correlation
            sentiment_normalized = max(-1.0, min(1.0, sentiment_change))
            
            # Price change normalized to -1 to +1 (assuming max 10% move = 1.0)
            price_change_normalized = max(-1.0, min(1.0, price_change_7d / 10.0))
            
            # Correlation: positive if both move same direction
            if abs(sentiment_normalized) < 0.1 or abs(price_change_normalized) < 0.01:
                correlation = 0.0  # Neutral
            else:
                correlation = (sentiment_normalized * price_change_normalized) / (abs(sentiment_normalized) * abs(price_change_normalized))
            
            # Detect divergence
            divergence = False
            divergence_type = None
            
            # BULLISH_DIVERGENCE: Price down but sentiment up (accumulation)
            if price_change_7d < -2.0 and sentiment_normalized > 0.2:
                divergence = True
                divergence_type = 'BULLISH_DIV'
            
            # BEARISH_DIVERGENCE: Price up but sentiment down (distribution)
            elif price_change_7d > 2.0 and sentiment_normalized < -0.2:
                divergence = True
                divergence_type = 'BEARISH_DIV'
            
            # Determine confirmation
            if divergence:
                confirmation = 'DIVERGENT'
            elif correlation > 0.5:
                confirmation = 'CONFIRMED'
            elif correlation < -0.5:
                confirmation = 'CONFIRMED'  # Negative correlation is also confirmation (inverse relationship)
            else:
                confirmation = 'NEUTRAL'
            
            return {
                'price_change_24h': price_change_24h,
                'price_change_7d': price_change_7d,
                'sentiment_price_correlation': correlation,
                'divergence': divergence,
                'divergence_type': divergence_type,
                'confirmation': confirmation,
                'current_price': current_price,
                'price_24h_ago': price_24h_ago,
                'price_7d_ago': price_7d_ago,
            }
            
        except Exception as e:
            logger.warning(f"Error correlating price for {symbol}: {e}")
            return {
                'price_change_24h': 0.0,
                'price_change_7d': 0.0,
                'sentiment_price_correlation': 0.0,
                'divergence': False,
                'divergence_type': None,
                'confirmation': 'NEUTRAL',
                'current_price': 0.0,
                'price_24h_ago': 0.0,
                'price_7d_ago': 0.0,
            }
    
    def _generate_signal(self,
                         avg_sentiment: float,
                         bullish_pct: float,
                         bearish_pct: float,
                         mention_change_pct: float,
                         wsb_dominance: float,
                         sentiment_trend: str,
                         velocity_surge: bool = False,
                         velocity_data: Optional[Dict] = None) -> Tuple[Optional[RedditSignalType], float, str, List[str], List[str]]:
        """
        Generate trading signal from Reddit metrics
        
        Returns:
            (signal_type, strength, action, reasoning, warnings)
        """
        reasoning = []
        warnings = []
        signal_strength = 50.0  # Base
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 1: FADE THE HYPE (Contrarian SHORT)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if avg_sentiment > self.FADE_HYPE_THRESHOLD and bullish_pct > self.BULLISH_PCT_EXTREME:
            signal_type = RedditSignalType.FADE_HYPE
            action = "SHORT"
            
            # Calculate strength
            sentiment_score = min((avg_sentiment - 0.3) * 100, 30)  # Up to 30 pts
            bullish_score = min((bullish_pct - 50) * 0.5, 20)  # Up to 20 pts
            signal_strength = 50 + sentiment_score + bullish_score
            
            reasoning.append(f"Extreme bullish sentiment ({avg_sentiment:+.2f})")
            reasoning.append(f"Crowd is {bullish_pct:.0f}% bullish - too crowded")
            reasoning.append("Contrarian signal: FADE THE HYPE")
            
            if wsb_dominance > self.WSB_DOMINANCE_WARNING:
                warnings.append(f"âš ï¸ High WSB dominance ({wsb_dominance:.0f}%) - meme risk")
                signal_strength -= 10
            
            return signal_type, min(signal_strength, 95), action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 2: FADE THE FEAR (Contrarian LONG)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if avg_sentiment < self.FADE_FEAR_THRESHOLD and bearish_pct > self.BEARISH_PCT_EXTREME:
            signal_type = RedditSignalType.FADE_FEAR
            action = "LONG"
            
            # Calculate strength
            sentiment_score = min(abs(avg_sentiment + 0.2) * 100, 30)
            bearish_score = min((bearish_pct - 40) * 0.5, 20)
            signal_strength = 50 + sentiment_score + bearish_score
            
            reasoning.append(f"Extreme bearish sentiment ({avg_sentiment:+.2f})")
            reasoning.append(f"Crowd is {bearish_pct:.0f}% bearish - fear is high")
            reasoning.append("Contrarian signal: FADE THE FEAR")
            
            return signal_type, min(signal_strength, 95), action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 3: MOMENTUM SURGE (Follow the crowd)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if mention_change_pct > 100 and sentiment_trend == "IMPROVING":
            signal_type = RedditSignalType.MOMENTUM_SURGE
            action = "LONG" if avg_sentiment > 0 else "SHORT"
            
            momentum_score = min(mention_change_pct / 10, 30)
            trend_score = 15 if sentiment_trend == "IMPROVING" else 5
            signal_strength = 50 + momentum_score + trend_score
            
            reasoning.append(f"Mention surge: {mention_change_pct:+.0f}% vs yesterday")
            reasoning.append(f"Sentiment trend: {sentiment_trend}")
            reasoning.append("Momentum play - ride the wave")
            
            warnings.append("âš ï¸ High volatility expected")
            
            return signal_type, min(signal_strength, 90), action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 4: PUMP WARNING (Avoid)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if mention_change_pct > 200 and wsb_dominance > 60:
            signal_type = RedditSignalType.PUMP_WARNING
            action = "AVOID"
            signal_strength = 80
            
            reasoning.append(f"Massive mention spike: {mention_change_pct:+.0f}%")
            reasoning.append(f"WSB dominance: {wsb_dominance:.0f}%")
            reasoning.append("ğŸš¨ PUMP & DUMP WARNING - Stay away!")
            
            warnings.append("âš ï¸ High manipulation risk")
            warnings.append("âš ï¸ Retail FOMO in progress")
            
            return signal_type, signal_strength, action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 5: SENTIMENT FLIP (Trend change)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if sentiment_trend == "IMPROVING" and avg_sentiment > 0.1:
            signal_type = RedditSignalType.SENTIMENT_FLIP
            action = "WATCH_LONG"
            signal_strength = 55
            
            reasoning.append(f"Sentiment improving: {sentiment_trend}")
            reasoning.append(f"Current sentiment: {avg_sentiment:+.2f}")
            reasoning.append("Potential trend change - watch for entry")
            
            return signal_type, signal_strength, action, reasoning, warnings
        
        if sentiment_trend == "DECLINING" and avg_sentiment < -0.1:
            signal_type = RedditSignalType.SENTIMENT_FLIP
            action = "WATCH_SHORT"
            signal_strength = 55
            
            reasoning.append(f"Sentiment declining: {sentiment_trend}")
            reasoning.append(f"Current sentiment: {avg_sentiment:+.2f}")
            reasoning.append("Potential trend change - watch for short entry")
            
            return signal_type, signal_strength, action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # NO SIGNAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        reasoning.append(f"Sentiment: {avg_sentiment:+.2f} (neutral)")
        reasoning.append(f"No clear contrarian edge")
        
        return None, 40, "NEUTRAL", reasoning, warnings
    
    def discover_hot_tickers(self, min_sentiment_extreme: float = 0.3, max_tickers: Optional[int] = None) -> List[HotTickerDiscovery]:
        """
        Scan universe to discover hot tickers with rate limit management.
        
        Args:
            min_sentiment_extreme: Minimum absolute sentiment for "hot"
            max_tickers: Maximum tickers to scan (None = all, respects rate limits)
        
        Returns:
            List of HotTickerDiscovery sorted by momentum
        """
        logger.info("ğŸ”¥ Scanning for HOT tickers on Reddit...")
        
        # Sort by priority (highest first)
        sorted_universe = sorted(
            self.scan_universe,
            key=lambda s: self._get_ticker_priority(s),
            reverse=True
        )
        
        # Limit tickers if specified or if rate limit is high
        remaining_requests = self.rate_limit_per_minute - len(self.request_times)
        if max_tickers is None:
            # Auto-limit based on rate limit
            # Each ticker = 1 request (1 page), leave 20% buffer
            safe_limit = int(remaining_requests * 0.8)
            max_tickers = min(safe_limit, len(sorted_universe))
            logger.debug(f"   Auto-limiting to {max_tickers} tickers (rate limit: {remaining_requests} remaining)")
        
        discoveries = []
        scanned = 0
        skipped_due_to_rate_limit = 0
        
        for symbol in sorted_universe:
            # Check rate limit before each ticker
            if not self._check_rate_limit():
                skipped_due_to_rate_limit += 1
                logger.debug(f"   â­ï¸ Skipping {symbol} (rate limit reached)")
                continue
            
            if max_tickers and scanned >= max_tickers:
                logger.debug(f"   â­ï¸ Reached max_tickers limit ({max_tickers})")
                break
            
            try:
                mentions = self._fetch_mentions(symbol, days=3, max_pages=1)
                scanned += 1
                
                if not mentions:
                    continue
                
                # Calculate metrics
                sentiments = [m.sentiment for m in mentions]
                avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
                bullish_pct = (len([s for s in sentiments if s > 0.3]) / len(sentiments)) * 100
                
                wsb_count = len([m for m in mentions if m.subreddit == 'wallstreetbets'])
                
                # Momentum score (higher = more activity)
                momentum_score = len(mentions) * (1 + abs(avg_sentiment))
                
                # Determine discovery reason
                if abs(avg_sentiment) > min_sentiment_extreme:
                    if avg_sentiment > 0:
                        reason = f"ğŸ”¥ BULLISH ({avg_sentiment:+.2f})"
                    else:
                        reason = f"â„ï¸ BEARISH ({avg_sentiment:+.2f})"
                elif wsb_count > 50:
                    reason = f"ğŸ° WSB HOT ({wsb_count} posts)"
                elif len(mentions) >= 100:
                    reason = f"ğŸ“ˆ HIGH VOLUME ({len(mentions)} posts)"
                else:
                    continue  # Skip if not interesting
                
                discovery = HotTickerDiscovery(
                    symbol=symbol,
                    mention_count=len(mentions),
                    avg_sentiment=avg_sentiment,
                    bullish_pct=bullish_pct,
                    wsb_mentions=wsb_count,
                    momentum_score=momentum_score,
                    discovery_reason=reason
                )
                
                discoveries.append(discovery)
                
            except Exception as e:
                logger.debug(f"Error scanning {symbol}: {e}")
        
        # Sort by momentum score
        discoveries.sort(key=lambda x: x.momentum_score, reverse=True)
        
        logger.info(f"âœ… Found {len(discoveries)} hot tickers (scanned {scanned}, skipped {skipped_due_to_rate_limit} due to rate limit)")
        
        return discoveries
    
    def get_contrarian_signals(self, min_strength: float = 60, max_tickers: Optional[int] = None) -> List[RedditTickerAnalysis]:
        """
        Get all actionable contrarian signals from universe with rate limit management.
        
        Args:
            min_strength: Minimum signal strength
            max_tickers: Maximum tickers to scan (None = all, respects rate limits)
        
        Returns:
            List of RedditTickerAnalysis with signals
        """
        logger.info("ğŸ¯ Scanning for contrarian signals...")
        
        # Sort by priority (highest first)
        sorted_universe = sorted(
            self.scan_universe,
            key=lambda s: self._get_ticker_priority(s),
            reverse=True
        )
        
        # Limit tickers if specified or if rate limit is high
        remaining_requests = self.rate_limit_per_minute - len(self.request_times)
        if max_tickers is None:
            # Auto-limit based on rate limit
            # Each ticker = 3 requests (3 pages), leave 20% buffer
            safe_limit = int((remaining_requests / 3) * 0.8)
            max_tickers = min(safe_limit, len(sorted_universe))
            logger.debug(f"   Auto-limiting to {max_tickers} tickers (rate limit: {remaining_requests} remaining)")
        
        signals = []
        scanned = 0
        skipped_due_to_rate_limit = 0
        
        for symbol in sorted_universe:
            # Check rate limit before each ticker
            if not self._check_rate_limit():
                skipped_due_to_rate_limit += 1
                logger.debug(f"   â­ï¸ Skipping {symbol} (rate limit reached)")
                continue
            
            if max_tickers and scanned >= max_tickers:
                logger.debug(f"   â­ï¸ Reached max_tickers limit ({max_tickers})")
                break
            
            analysis = self.analyze_ticker(symbol, days=3)
            scanned += 1
            
            if analysis and analysis.signal_type and analysis.signal_strength >= min_strength:
                if analysis.action not in ["NEUTRAL", "WATCH_LONG", "WATCH_SHORT"]:
                    signals.append(analysis)
        
        # Sort by signal strength
        signals.sort(key=lambda x: x.signal_strength, reverse=True)
        
        logger.info(f"âœ… Found {len(signals)} actionable signals (scanned {scanned}, skipped {skipped_due_to_rate_limit} due to rate limit)")
        
        return signals
    
    def print_analysis(self, analysis: RedditTickerAnalysis):
        """Print human-readable analysis"""
        print("\n" + "=" * 80)
        print(f"ğŸ“± REDDIT ANALYSIS: {analysis.symbol}")
        print("=" * 80)
        
        print(f"\nğŸ“Š MENTIONS:")
        print(f"   Total: {analysis.total_mentions}")
        print(f"   Today: {analysis.mentions_today}")
        print(f"   Yesterday: {analysis.mentions_24h_ago}")
        print(f"   Change: {analysis.mention_change_pct:+.0f}%")
        
        print(f"\nğŸ“ˆ SENTIMENT:")
        print(f"   Average: {analysis.avg_sentiment:+.3f}")
        print(f"   Bullish: {analysis.bullish_pct:.0f}%")
        print(f"   Bearish: {analysis.bearish_pct:.0f}%")
        print(f"   Trend: {analysis.sentiment_trend}")
        
        print(f"\nğŸ“± SUBREDDITS:")
        for sub, count in analysis.top_subreddits.items():
            print(f"   r/{sub}: {count}")
        print(f"   WSB Dominance: {analysis.wsb_dominance:.0f}%")
        
        print(f"\nğŸ¯ SIGNAL:")
        if analysis.signal_type:
            print(f"   Type: {analysis.signal_type.value}")
        print(f"   Action: {analysis.action}")
        print(f"   Strength: {analysis.signal_strength:.0f}%")
        
        print(f"\nğŸ’¡ REASONING:")
        for r in analysis.reasoning:
            print(f"   â€¢ {r}")
        
        if analysis.warnings:
            print(f"\nâš ï¸ WARNINGS:")
            for w in analysis.warnings:
                print(f"   {w}")
        
        print("\n" + "=" * 80)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
    
    from chartexchange_config import CHARTEXCHANGE_API_KEY
    
    exploiter = RedditExploiter(api_key=CHARTEXCHANGE_API_KEY)
    
    print("\n" + "=" * 80)
    print("ğŸ”¥ REDDIT EXPLOITER TEST")
    print("=" * 80)
    
    # Test 1: Analyze single ticker
    print("\nğŸ“± TEST 1: Analyzing TSLA...")
    analysis = exploiter.analyze_ticker("TSLA", days=3)
    if analysis:
        exploiter.print_analysis(analysis)
    
    # Test 2: Discover hot tickers
    print("\nğŸ”¥ TEST 2: Discovering hot tickers...")
    hot_tickers = exploiter.discover_hot_tickers()
    
    print(f"\n{'Rank':<5} {'Ticker':<8} {'Posts':<8} {'Sentiment':<12} {'Reason'}")
    print("-" * 60)
    for i, t in enumerate(hot_tickers[:15], 1):
        print(f"{i:<5} {t.symbol:<8} {t.mention_count:<8} {t.avg_sentiment:+.3f}        {t.discovery_reason}")
    
    # Test 3: Get contrarian signals
    print("\nğŸ¯ TEST 3: Getting contrarian signals...")
    signals = exploiter.get_contrarian_signals(min_strength=55)
    
    if signals:
        print(f"\n{'Ticker':<8} {'Signal':<15} {'Action':<8} {'Strength':<10} {'Reasoning'}")
        print("-" * 80)
        for s in signals[:10]:
            signal_name = s.signal_type.value if s.signal_type else "NONE"
            reason = s.reasoning[0] if s.reasoning else ""
            print(f"{s.symbol:<8} {signal_name:<15} {s.action:<8} {s.signal_strength:<10.0f} {reason[:40]}")
    else:
        print("   No actionable signals found")
    
    print("\nâœ… Reddit Exploiter test complete!")

