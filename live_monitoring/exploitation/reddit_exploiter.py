#!/usr/bin/env python3
"""
ğŸ”¥ REDDIT EXPLOITER - Phase 5 Exploitation Module
==================================================

Exploits Reddit sentiment for contrarian trading signals.

KEY CAPABILITIES:
1. HOT TICKER DISCOVERY - Find what retail is talking about
2. SENTIMENT MOMENTUM - Track sentiment changes over time
3. CONTRARIAN SIGNALS - Fade the hype, fade the fear
4. STEALTH DETECTION - Find tickers with low noise (smart money)

DATA SOURCE: ChartExchange Reddit API
- 774K+ TSLA mentions available
- Real-time sentiment scoring
- Subreddit breakdown
- Pagination support

Author: Alpha's AI Hedge Fund
Date: 2024-12-16
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass, field
from enum import Enum
from collections import Counter
import requests
import time

# Add paths
sys.path.append(str(Path(__file__).parent.parent.parent / 'core/data'))
sys.path.append(str(Path(__file__).parent.parent.parent / 'configs'))

logger = logging.getLogger(__name__)


class RedditSignalType(Enum):
    """Types of Reddit-based signals"""
    FADE_HYPE = "FADE_HYPE"          # Extreme bullish = SHORT signal
    FADE_FEAR = "FADE_FEAR"          # Extreme bearish = LONG signal
    MOMENTUM_SURGE = "MOMENTUM_SURGE" # Rapid mention increase = momentum play
    STEALTH_MOVE = "STEALTH_MOVE"    # Low mentions + price move = smart money
    PUMP_WARNING = "PUMP_WARNING"    # Sudden spike = potential pump & dump
    SENTIMENT_FLIP = "SENTIMENT_FLIP" # Sentiment reversal = trend change


@dataclass
class RedditMention:
    """Single Reddit mention with sentiment"""
    subreddit: str
    created: datetime
    sentiment: float  # -1 to +1
    author: str
    text: str
    link: str
    thing_type: str  # 'comment' or 'submission'


@dataclass
class RedditTickerAnalysis:
    """Complete Reddit analysis for a ticker"""
    symbol: str
    timestamp: datetime
    
    # Mention metrics
    total_mentions: int
    mentions_today: int
    mentions_24h_ago: int
    mention_change_pct: float
    
    # Sentiment metrics
    avg_sentiment: float
    bullish_pct: float  # % of posts with sentiment > 0.3
    bearish_pct: float  # % of posts with sentiment < -0.3
    sentiment_trend: str  # "IMPROVING", "DECLINING", "STABLE"
    
    # Subreddit breakdown
    top_subreddits: Dict[str, int]
    wsb_dominance: float  # % from r/wallstreetbets
    
    # Signal
    signal_type: Optional[RedditSignalType]
    signal_strength: float  # 0-100
    action: str  # "LONG", "SHORT", "AVOID", "WATCH"
    
    # Context
    reasoning: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    sample_posts: List[str] = field(default_factory=list)


@dataclass
class HotTickerDiscovery:
    """Hot ticker discovered from Reddit"""
    symbol: str
    mention_count: int
    avg_sentiment: float
    bullish_pct: float
    wsb_mentions: int
    momentum_score: float  # How fast mentions are growing
    discovery_reason: str


class RedditExploiter:
    """
    ğŸ”¥ Reddit Exploitation Engine
    
    Scans Reddit for trading opportunities by:
    1. Finding HOT tickers (what retail is talking about)
    2. Tracking sentiment momentum (bullish/bearish shifts)
    3. Generating contrarian signals (fade hype/fear)
    4. Detecting stealth moves (smart money accumulation)
    """
    
    # Thresholds
    FADE_HYPE_THRESHOLD = 0.4      # Avg sentiment > 0.4 = fade
    FADE_FEAR_THRESHOLD = -0.3    # Avg sentiment < -0.3 = buy fear
    BULLISH_PCT_EXTREME = 60      # > 60% bullish = too crowded
    BEARISH_PCT_EXTREME = 50      # > 50% bearish = fear
    MENTION_SURGE_THRESHOLD = 2.0  # 2x normal = surge
    WSB_DOMINANCE_WARNING = 70    # > 70% from WSB = meme risk
    
    # Signal strength weights
    SENTIMENT_WEIGHT = 40
    MOMENTUM_WEIGHT = 30
    VOLUME_WEIGHT = 20
    CONTRARIAN_WEIGHT = 10
    
    def __init__(self, api_key: str):
        """
        Initialize Reddit Exploiter
        
        Args:
            api_key: ChartExchange API key
        """
        self.api_key = api_key
        self.base_url = "https://chartexchange.com/api/v1"
        self.session = requests.Session()
        
        # Rate limiting (Tier 3: 1000 requests/minute)
        self.rate_limit_per_minute = 1000
        self.request_times = []  # Track request timestamps
        self.request_lock = False  # Lock to prevent concurrent rate limit checks
        
        # Cache for rate limiting
        self.cache = {}
        self.cache_ttl = 300  # 5 minutes
        
        # Priority tiers for tickers (higher = more important)
        self.ticker_priorities = {
            # Tier 1: Always scan (highest priority)
            'TSLA': 10, 'NVDA': 10, 'AAPL': 10, 'MSFT': 10, 'SPY': 10, 'QQQ': 10,
            # Tier 2: High priority
            'GME': 8, 'AMC': 8, 'AMD': 8, 'GOOGL': 8, 'AMZN': 8, 'META': 8,
            # Tier 3: Medium priority
            'PLTR': 6, 'SOFI': 6, 'MSTR': 6, 'COIN': 6, 'RIOT': 6, 'MARA': 6,
            # Tier 4: Lower priority (default)
        }
        
        # Ticker universe for scanning
        self.scan_universe = [
            # Mega caps (always relevant)
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA',
            # Semiconductors (AI plays)
            'AMD', 'INTC', 'AVGO', 'MU', 'SMCI', 'ARM', 'QCOM',
            # Meme favorites
            'GME', 'AMC', 'BB', 'KOSS', 'BBBY',
            # Crypto-related
            'MSTR', 'COIN', 'MARA', 'RIOT', 'HUT',
            # AI/Cloud
            'PLTR', 'AI', 'SNOW', 'DDOG', 'NET', 'CRM',
            # EVs
            'RIVN', 'LCID', 'NIO', 'XPEV', 'LI',
            # Fintech
            'SOFI', 'HOOD', 'AFRM', 'UPST', 'SQ', 'PYPL',
            # Other popular
            'ROKU', 'SHOP', 'ZM', 'DOCU', 'CRWD', 'ZS'
        ]
        
        logger.info("ğŸ”¥ Reddit Exploiter initialized")
        logger.info(f"   Rate limit: {self.rate_limit_per_minute} requests/minute")
        logger.info(f"   Cache TTL: {self.cache_ttl} seconds")
        logger.info(f"   Ticker universe: {len(self.scan_universe)} tickers")
    
    def _check_rate_limit(self) -> bool:
        """
        Check if we can make a request without hitting rate limit.
        
        Returns:
            True if request allowed, False if rate limit would be exceeded
        """
        now = datetime.now()
        
        # Clean old requests (older than 1 minute)
        cutoff = now - timedelta(minutes=1)
        self.request_times = [t for t in self.request_times if t > cutoff]
        
        # Check if we're at limit
        if len(self.request_times) >= self.rate_limit_per_minute:
            logger.warning(f"âš ï¸ Rate limit reached: {len(self.request_times)}/{self.rate_limit_per_minute} requests in last minute")
            return False
        
        return True
    
    def _wait_for_rate_limit(self, max_wait_seconds: int = 5):
        """
        Wait until rate limit allows a request.
        
        Args:
            max_wait_seconds: Maximum seconds to wait
        """
        waited = 0
        while not self._check_rate_limit() and waited < max_wait_seconds:
            time.sleep(0.5)
            waited += 0.5
        
        if waited >= max_wait_seconds:
            logger.warning(f"âš ï¸ Rate limit wait timeout after {max_wait_seconds}s")
    
    def _record_request(self):
        """Record a request timestamp for rate limiting."""
        self.request_times.append(datetime.now())
    
    def _get_ticker_priority(self, symbol: str) -> int:
        """Get priority for a ticker (higher = more important)."""
        return self.ticker_priorities.get(symbol, 5)  # Default priority: 5
    
    def get_rate_limit_status(self) -> Dict:
        """
        Get current rate limit status.
        
        Returns:
            {
                'requests_last_minute': int,
                'rate_limit': int,
                'remaining': int,
                'utilization_pct': float,
                'can_make_request': bool
            }
        """
        now = datetime.now()
        cutoff = now - timedelta(minutes=1)
        recent_requests = [t for t in self.request_times if t > cutoff]
        
        requests_count = len(recent_requests)
        remaining = self.rate_limit_per_minute - requests_count
        utilization = (requests_count / self.rate_limit_per_minute) * 100
        
        return {
            'requests_last_minute': requests_count,
            'rate_limit': self.rate_limit_per_minute,
            'remaining': remaining,
            'utilization_pct': utilization,
            'can_make_request': remaining > 0
        }
    
    def _fetch_mentions(self, symbol: str, days: int = 7, max_pages: int = 3) -> List[RedditMention]:
        """
        Fetch Reddit mentions with pagination support and rate limiting.
        
        Args:
            symbol: Ticker symbol
            days: Number of days to fetch
            max_pages: Maximum pages to fetch (100 per page)
        
        Returns:
            List of RedditMention objects
        """
        cache_key = f"{symbol}_{days}_{max_pages}"
        
        # Check cache first (avoids API call)
        if cache_key in self.cache:
            cached_time, cached_data = self.cache[cache_key]
            if (datetime.now() - cached_time).seconds < self.cache_ttl:
                logger.debug(f"ğŸ“¦ Cache hit for {symbol}")
                return cached_data
        
        # Check rate limit before making request
        if not self._check_rate_limit():
            logger.warning(f"âš ï¸ Rate limit reached, skipping {symbol} (will use cache if available)")
            # Return cached data even if expired (better than nothing)
            if cache_key in self.cache:
                cached_time, cached_data = self.cache[cache_key]
                logger.debug(f"   Using expired cache for {symbol}")
                return cached_data
            return []
        
        mentions = []
        next_url = f"{self.base_url}/data/reddit/mentions/stock/{symbol.upper()}/"
        params = {
            'api_key': self.api_key,
            'days': days
        }
        
        pages_fetched = 0
        
        while next_url and pages_fetched < max_pages:
            # Check rate limit before each page
            if not self._check_rate_limit():
                logger.warning(f"âš ï¸ Rate limit reached while fetching {symbol} (got {pages_fetched} pages)")
                break
            
            try:
                if pages_fetched == 0:
                    resp = self.session.get(next_url, params=params, timeout=10)
                else:
                    # Next URL already has params
                    resp = self.session.get(next_url, timeout=10)
                
                # Record request
                self._record_request()
                
                if resp.status_code == 429:  # Rate limit exceeded
                    logger.warning(f"âš ï¸ API returned 429 (rate limit) for {symbol}")
                    self._wait_for_rate_limit(max_wait_seconds=5)
                    continue
                
                if resp.status_code != 200:
                    logger.warning(f"Reddit API error for {symbol}: {resp.status_code}")
                    break
                
                data = resp.json()
                
                # Handle paginated response
                if isinstance(data, dict):
                    results = data.get('results', [])
                    next_url = data.get('next')
                else:
                    results = data
                    next_url = None
                
                for item in results:
                    try:
                        created = datetime.strptime(item['created'], '%Y-%m-%d %H:%M:%S')
                        mention = RedditMention(
                            subreddit=item.get('subreddit', 'unknown'),
                            created=created,
                            sentiment=float(item.get('sentiment', 0)),
                            author=item.get('author', 'unknown'),
                            text=item.get('text', '')[:500],  # Truncate
                            link=item.get('link', ''),
                            thing_type=item.get('thing_type', 'comment')
                        )
                        mentions.append(mention)
                    except Exception as e:
                        logger.debug(f"Error parsing mention: {e}")
                
                pages_fetched += 1
                
            except Exception as e:
                logger.error(f"Error fetching Reddit mentions for {symbol}: {e}")
                break
        
        # Update cache
        self.cache[cache_key] = (datetime.now(), mentions)
        
        return mentions
    
    def analyze_ticker(self, symbol: str, days: int = 3) -> Optional[RedditTickerAnalysis]:
        """
        Comprehensive Reddit analysis for a single ticker
        
        Args:
            symbol: Ticker symbol
            days: Days to analyze
        
        Returns:
            RedditTickerAnalysis or None
        """
        try:
            logger.info(f"ğŸ“± Analyzing Reddit sentiment for {symbol}...")
            
            # Fetch mentions (up to 300 posts)
            mentions = self._fetch_mentions(symbol, days=days, max_pages=3)
            
            if not mentions:
                logger.warning(f"No Reddit data for {symbol}")
                return None
            
            # Calculate metrics
            total_mentions = len(mentions)
            
            # Split by time
            now = datetime.now()
            today = now.replace(hour=0, minute=0, second=0, microsecond=0)
            yesterday = today - timedelta(days=1)
            
            mentions_today = len([m for m in mentions if m.created >= today])
            mentions_yesterday = len([m for m in mentions if yesterday <= m.created < today])
            
            # Mention change
            if mentions_yesterday > 0:
                mention_change_pct = ((mentions_today - mentions_yesterday) / mentions_yesterday) * 100
            else:
                mention_change_pct = 100.0 if mentions_today > 0 else 0.0
            
            # Sentiment analysis
            sentiments = [m.sentiment for m in mentions]
            avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
            
            bullish_count = len([s for s in sentiments if s > 0.3])
            bearish_count = len([s for s in sentiments if s < -0.3])
            
            bullish_pct = (bullish_count / total_mentions) * 100
            bearish_pct = (bearish_count / total_mentions) * 100
            
            # Sentiment trend (compare first half vs second half)
            mid = len(sentiments) // 2
            if mid > 0:
                first_half_avg = sum(sentiments[:mid]) / mid
                second_half_avg = sum(sentiments[mid:]) / (len(sentiments) - mid)
                
                if second_half_avg > first_half_avg + 0.1:
                    sentiment_trend = "IMPROVING"
                elif second_half_avg < first_half_avg - 0.1:
                    sentiment_trend = "DECLINING"
                else:
                    sentiment_trend = "STABLE"
            else:
                sentiment_trend = "STABLE"
            
            # Subreddit breakdown
            subreddit_counts = Counter([m.subreddit for m in mentions])
            top_subreddits = dict(subreddit_counts.most_common(5))
            
            wsb_count = subreddit_counts.get('wallstreetbets', 0)
            wsb_dominance = (wsb_count / total_mentions) * 100 if total_mentions > 0 else 0
            
            # Generate signal
            signal_type, signal_strength, action, reasoning, warnings = self._generate_signal(
                avg_sentiment=avg_sentiment,
                bullish_pct=bullish_pct,
                bearish_pct=bearish_pct,
                mention_change_pct=mention_change_pct,
                wsb_dominance=wsb_dominance,
                sentiment_trend=sentiment_trend
            )
            
            # Sample posts
            sample_posts = [
                f"[{m.sentiment:+.2f}] r/{m.subreddit}: {m.text[:100]}..."
                for m in sorted(mentions, key=lambda x: abs(x.sentiment), reverse=True)[:3]
            ]
            
            analysis = RedditTickerAnalysis(
                symbol=symbol,
                timestamp=datetime.now(),
                total_mentions=total_mentions,
                mentions_today=mentions_today,
                mentions_24h_ago=mentions_yesterday,
                mention_change_pct=mention_change_pct,
                avg_sentiment=avg_sentiment,
                bullish_pct=bullish_pct,
                bearish_pct=bearish_pct,
                sentiment_trend=sentiment_trend,
                top_subreddits=top_subreddits,
                wsb_dominance=wsb_dominance,
                signal_type=signal_type,
                signal_strength=signal_strength,
                action=action,
                reasoning=reasoning,
                warnings=warnings,
                sample_posts=sample_posts
            )
            
            logger.info(f"âœ… Reddit analysis complete for {symbol}: {action} ({signal_strength:.0f}%)")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing {symbol}: {e}")
            return None
    
    def _generate_signal(self,
                         avg_sentiment: float,
                         bullish_pct: float,
                         bearish_pct: float,
                         mention_change_pct: float,
                         wsb_dominance: float,
                         sentiment_trend: str) -> Tuple[Optional[RedditSignalType], float, str, List[str], List[str]]:
        """
        Generate trading signal from Reddit metrics
        
        Returns:
            (signal_type, strength, action, reasoning, warnings)
        """
        reasoning = []
        warnings = []
        signal_strength = 50.0  # Base
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 1: FADE THE HYPE (Contrarian SHORT)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if avg_sentiment > self.FADE_HYPE_THRESHOLD and bullish_pct > self.BULLISH_PCT_EXTREME:
            signal_type = RedditSignalType.FADE_HYPE
            action = "SHORT"
            
            # Calculate strength
            sentiment_score = min((avg_sentiment - 0.3) * 100, 30)  # Up to 30 pts
            bullish_score = min((bullish_pct - 50) * 0.5, 20)  # Up to 20 pts
            signal_strength = 50 + sentiment_score + bullish_score
            
            reasoning.append(f"Extreme bullish sentiment ({avg_sentiment:+.2f})")
            reasoning.append(f"Crowd is {bullish_pct:.0f}% bullish - too crowded")
            reasoning.append("Contrarian signal: FADE THE HYPE")
            
            if wsb_dominance > self.WSB_DOMINANCE_WARNING:
                warnings.append(f"âš ï¸ High WSB dominance ({wsb_dominance:.0f}%) - meme risk")
                signal_strength -= 10
            
            return signal_type, min(signal_strength, 95), action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 2: FADE THE FEAR (Contrarian LONG)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if avg_sentiment < self.FADE_FEAR_THRESHOLD and bearish_pct > self.BEARISH_PCT_EXTREME:
            signal_type = RedditSignalType.FADE_FEAR
            action = "LONG"
            
            # Calculate strength
            sentiment_score = min(abs(avg_sentiment + 0.2) * 100, 30)
            bearish_score = min((bearish_pct - 40) * 0.5, 20)
            signal_strength = 50 + sentiment_score + bearish_score
            
            reasoning.append(f"Extreme bearish sentiment ({avg_sentiment:+.2f})")
            reasoning.append(f"Crowd is {bearish_pct:.0f}% bearish - fear is high")
            reasoning.append("Contrarian signal: FADE THE FEAR")
            
            return signal_type, min(signal_strength, 95), action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 3: MOMENTUM SURGE (Follow the crowd)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if mention_change_pct > 100 and sentiment_trend == "IMPROVING":
            signal_type = RedditSignalType.MOMENTUM_SURGE
            action = "LONG" if avg_sentiment > 0 else "SHORT"
            
            momentum_score = min(mention_change_pct / 10, 30)
            trend_score = 15 if sentiment_trend == "IMPROVING" else 5
            signal_strength = 50 + momentum_score + trend_score
            
            reasoning.append(f"Mention surge: {mention_change_pct:+.0f}% vs yesterday")
            reasoning.append(f"Sentiment trend: {sentiment_trend}")
            reasoning.append("Momentum play - ride the wave")
            
            warnings.append("âš ï¸ High volatility expected")
            
            return signal_type, min(signal_strength, 90), action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 4: PUMP WARNING (Avoid)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if mention_change_pct > 200 and wsb_dominance > 60:
            signal_type = RedditSignalType.PUMP_WARNING
            action = "AVOID"
            signal_strength = 80
            
            reasoning.append(f"Massive mention spike: {mention_change_pct:+.0f}%")
            reasoning.append(f"WSB dominance: {wsb_dominance:.0f}%")
            reasoning.append("ğŸš¨ PUMP & DUMP WARNING - Stay away!")
            
            warnings.append("âš ï¸ High manipulation risk")
            warnings.append("âš ï¸ Retail FOMO in progress")
            
            return signal_type, signal_strength, action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SIGNAL 5: SENTIMENT FLIP (Trend change)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if sentiment_trend == "IMPROVING" and avg_sentiment > 0.1:
            signal_type = RedditSignalType.SENTIMENT_FLIP
            action = "WATCH_LONG"
            signal_strength = 55
            
            reasoning.append(f"Sentiment improving: {sentiment_trend}")
            reasoning.append(f"Current sentiment: {avg_sentiment:+.2f}")
            reasoning.append("Potential trend change - watch for entry")
            
            return signal_type, signal_strength, action, reasoning, warnings
        
        if sentiment_trend == "DECLINING" and avg_sentiment < -0.1:
            signal_type = RedditSignalType.SENTIMENT_FLIP
            action = "WATCH_SHORT"
            signal_strength = 55
            
            reasoning.append(f"Sentiment declining: {sentiment_trend}")
            reasoning.append(f"Current sentiment: {avg_sentiment:+.2f}")
            reasoning.append("Potential trend change - watch for short entry")
            
            return signal_type, signal_strength, action, reasoning, warnings
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # NO SIGNAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        reasoning.append(f"Sentiment: {avg_sentiment:+.2f} (neutral)")
        reasoning.append(f"No clear contrarian edge")
        
        return None, 40, "NEUTRAL", reasoning, warnings
    
    def discover_hot_tickers(self, min_sentiment_extreme: float = 0.3, max_tickers: Optional[int] = None) -> List[HotTickerDiscovery]:
        """
        Scan universe to discover hot tickers with rate limit management.
        
        Args:
            min_sentiment_extreme: Minimum absolute sentiment for "hot"
            max_tickers: Maximum tickers to scan (None = all, respects rate limits)
        
        Returns:
            List of HotTickerDiscovery sorted by momentum
        """
        logger.info("ğŸ”¥ Scanning for HOT tickers on Reddit...")
        
        # Sort by priority (highest first)
        sorted_universe = sorted(
            self.scan_universe,
            key=lambda s: self._get_ticker_priority(s),
            reverse=True
        )
        
        # Limit tickers if specified or if rate limit is high
        remaining_requests = self.rate_limit_per_minute - len(self.request_times)
        if max_tickers is None:
            # Auto-limit based on rate limit
            # Each ticker = 1 request (1 page), leave 20% buffer
            safe_limit = int(remaining_requests * 0.8)
            max_tickers = min(safe_limit, len(sorted_universe))
            logger.debug(f"   Auto-limiting to {max_tickers} tickers (rate limit: {remaining_requests} remaining)")
        
        discoveries = []
        scanned = 0
        skipped_due_to_rate_limit = 0
        
        for symbol in sorted_universe:
            # Check rate limit before each ticker
            if not self._check_rate_limit():
                skipped_due_to_rate_limit += 1
                logger.debug(f"   â­ï¸ Skipping {symbol} (rate limit reached)")
                continue
            
            if max_tickers and scanned >= max_tickers:
                logger.debug(f"   â­ï¸ Reached max_tickers limit ({max_tickers})")
                break
            
            try:
                mentions = self._fetch_mentions(symbol, days=3, max_pages=1)
                scanned += 1
                
                if not mentions:
                    continue
                
                # Calculate metrics
                sentiments = [m.sentiment for m in mentions]
                avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
                bullish_pct = (len([s for s in sentiments if s > 0.3]) / len(sentiments)) * 100
                
                wsb_count = len([m for m in mentions if m.subreddit == 'wallstreetbets'])
                
                # Momentum score (higher = more activity)
                momentum_score = len(mentions) * (1 + abs(avg_sentiment))
                
                # Determine discovery reason
                if abs(avg_sentiment) > min_sentiment_extreme:
                    if avg_sentiment > 0:
                        reason = f"ğŸ”¥ BULLISH ({avg_sentiment:+.2f})"
                    else:
                        reason = f"â„ï¸ BEARISH ({avg_sentiment:+.2f})"
                elif wsb_count > 50:
                    reason = f"ğŸ° WSB HOT ({wsb_count} posts)"
                elif len(mentions) >= 100:
                    reason = f"ğŸ“ˆ HIGH VOLUME ({len(mentions)} posts)"
                else:
                    continue  # Skip if not interesting
                
                discovery = HotTickerDiscovery(
                    symbol=symbol,
                    mention_count=len(mentions),
                    avg_sentiment=avg_sentiment,
                    bullish_pct=bullish_pct,
                    wsb_mentions=wsb_count,
                    momentum_score=momentum_score,
                    discovery_reason=reason
                )
                
                discoveries.append(discovery)
                
            except Exception as e:
                logger.debug(f"Error scanning {symbol}: {e}")
        
        # Sort by momentum score
        discoveries.sort(key=lambda x: x.momentum_score, reverse=True)
        
        logger.info(f"âœ… Found {len(discoveries)} hot tickers (scanned {scanned}, skipped {skipped_due_to_rate_limit} due to rate limit)")
        
        return discoveries
    
    def get_contrarian_signals(self, min_strength: float = 60, max_tickers: Optional[int] = None) -> List[RedditTickerAnalysis]:
        """
        Get all actionable contrarian signals from universe with rate limit management.
        
        Args:
            min_strength: Minimum signal strength
            max_tickers: Maximum tickers to scan (None = all, respects rate limits)
        
        Returns:
            List of RedditTickerAnalysis with signals
        """
        logger.info("ğŸ¯ Scanning for contrarian signals...")
        
        # Sort by priority (highest first)
        sorted_universe = sorted(
            self.scan_universe,
            key=lambda s: self._get_ticker_priority(s),
            reverse=True
        )
        
        # Limit tickers if specified or if rate limit is high
        remaining_requests = self.rate_limit_per_minute - len(self.request_times)
        if max_tickers is None:
            # Auto-limit based on rate limit
            # Each ticker = 3 requests (3 pages), leave 20% buffer
            safe_limit = int((remaining_requests / 3) * 0.8)
            max_tickers = min(safe_limit, len(sorted_universe))
            logger.debug(f"   Auto-limiting to {max_tickers} tickers (rate limit: {remaining_requests} remaining)")
        
        signals = []
        scanned = 0
        skipped_due_to_rate_limit = 0
        
        for symbol in sorted_universe:
            # Check rate limit before each ticker
            if not self._check_rate_limit():
                skipped_due_to_rate_limit += 1
                logger.debug(f"   â­ï¸ Skipping {symbol} (rate limit reached)")
                continue
            
            if max_tickers and scanned >= max_tickers:
                logger.debug(f"   â­ï¸ Reached max_tickers limit ({max_tickers})")
                break
            
            analysis = self.analyze_ticker(symbol, days=3)
            scanned += 1
            
            if analysis and analysis.signal_type and analysis.signal_strength >= min_strength:
                if analysis.action not in ["NEUTRAL", "WATCH_LONG", "WATCH_SHORT"]:
                    signals.append(analysis)
        
        # Sort by signal strength
        signals.sort(key=lambda x: x.signal_strength, reverse=True)
        
        logger.info(f"âœ… Found {len(signals)} actionable signals (scanned {scanned}, skipped {skipped_due_to_rate_limit} due to rate limit)")
        
        return signals
    
    def print_analysis(self, analysis: RedditTickerAnalysis):
        """Print human-readable analysis"""
        print("\n" + "=" * 80)
        print(f"ğŸ“± REDDIT ANALYSIS: {analysis.symbol}")
        print("=" * 80)
        
        print(f"\nğŸ“Š MENTIONS:")
        print(f"   Total: {analysis.total_mentions}")
        print(f"   Today: {analysis.mentions_today}")
        print(f"   Yesterday: {analysis.mentions_24h_ago}")
        print(f"   Change: {analysis.mention_change_pct:+.0f}%")
        
        print(f"\nğŸ“ˆ SENTIMENT:")
        print(f"   Average: {analysis.avg_sentiment:+.3f}")
        print(f"   Bullish: {analysis.bullish_pct:.0f}%")
        print(f"   Bearish: {analysis.bearish_pct:.0f}%")
        print(f"   Trend: {analysis.sentiment_trend}")
        
        print(f"\nğŸ“± SUBREDDITS:")
        for sub, count in analysis.top_subreddits.items():
            print(f"   r/{sub}: {count}")
        print(f"   WSB Dominance: {analysis.wsb_dominance:.0f}%")
        
        print(f"\nğŸ¯ SIGNAL:")
        if analysis.signal_type:
            print(f"   Type: {analysis.signal_type.value}")
        print(f"   Action: {analysis.action}")
        print(f"   Strength: {analysis.signal_strength:.0f}%")
        
        print(f"\nğŸ’¡ REASONING:")
        for r in analysis.reasoning:
            print(f"   â€¢ {r}")
        
        if analysis.warnings:
            print(f"\nâš ï¸ WARNINGS:")
            for w in analysis.warnings:
                print(f"   {w}")
        
        print("\n" + "=" * 80)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STANDALONE TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
    
    from chartexchange_config import CHARTEXCHANGE_API_KEY
    
    exploiter = RedditExploiter(api_key=CHARTEXCHANGE_API_KEY)
    
    print("\n" + "=" * 80)
    print("ğŸ”¥ REDDIT EXPLOITER TEST")
    print("=" * 80)
    
    # Test 1: Analyze single ticker
    print("\nğŸ“± TEST 1: Analyzing TSLA...")
    analysis = exploiter.analyze_ticker("TSLA", days=3)
    if analysis:
        exploiter.print_analysis(analysis)
    
    # Test 2: Discover hot tickers
    print("\nğŸ”¥ TEST 2: Discovering hot tickers...")
    hot_tickers = exploiter.discover_hot_tickers()
    
    print(f"\n{'Rank':<5} {'Ticker':<8} {'Posts':<8} {'Sentiment':<12} {'Reason'}")
    print("-" * 60)
    for i, t in enumerate(hot_tickers[:15], 1):
        print(f"{i:<5} {t.symbol:<8} {t.mention_count:<8} {t.avg_sentiment:+.3f}        {t.discovery_reason}")
    
    # Test 3: Get contrarian signals
    print("\nğŸ¯ TEST 3: Getting contrarian signals...")
    signals = exploiter.get_contrarian_signals(min_strength=55)
    
    if signals:
        print(f"\n{'Ticker':<8} {'Signal':<15} {'Action':<8} {'Strength':<10} {'Reasoning'}")
        print("-" * 80)
        for s in signals[:10]:
            signal_name = s.signal_type.value if s.signal_type else "NONE"
            reason = s.reasoning[0] if s.reasoning else ""
            print(f"{s.symbol:<8} {signal_name:<15} {s.action:<8} {s.signal_strength:<10.0f} {reason[:40]}")
    else:
        print("   No actionable signals found")
    
    print("\nâœ… Reddit Exploiter test complete!")

