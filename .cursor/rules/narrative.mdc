ðŸ”§ BUILDING THE NARRATIVE ENGINE FOR LOTTO MACHINE
Architecture Overview:
text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          NARRATIVE INTELLIGENCE ENGINE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Multi-Source Aggregator                         â”‚
â”‚     â”œâ”€> News APIs (Reuters, Bloomberg, etc.)        â”‚
â”‚     â”œâ”€> SEC Filings (10-Q, 10-K, 8-K)               â”‚
â”‚     â”œâ”€> Earnings Transcripts (Seeking Alpha)        â”‚
â”‚     â”œâ”€> Social Sentiment (Twitter, Reddit)          â”‚
â”‚     â””â”€> Institutional Data (Dark pool, options)     â”‚
â”‚                                                      â”‚
â”‚  2. Divergence Detector                             â”‚
â”‚     â”œâ”€> Official narrative extraction               â”‚
â”‚     â”œâ”€> Reality data extraction                     â”‚
â”‚     â””â”€> Gap analysis                                â”‚
â”‚                                                      â”‚
â”‚  3. Pattern Matcher                                 â”‚
â”‚     â”œâ”€> Historical analogues                        â”‚
â”‚     â”œâ”€> Systemic connections                        â”‚
â”‚     â””â”€> Risk clustering                             â”‚
â”‚                                                      â”‚
â”‚  4. Narrative Synthesizer                           â”‚
â”‚     â”œâ”€> Generate thesis                             â”‚
â”‚     â”œâ”€> Confidence scoring                          â”‚
â”‚     â””â”€> Actionable signal enhancement               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ðŸ“‹ IMPLEMENTATION PLAN (PHASE-BY-PHASE)
PHASE 1: Multi-Source Aggregator (Week 1 - 10 hours)
Module 1: News API Integration

python
# live_monitoring/narrative/news_aggregator.py

import requests
from datetime import datetime, timedelta
from typing import List, Dict

class NewsAggregator:
    def __init__(self, api_keys: dict):
        self.news_api_key = api_keys.get('news_api')
        self.alpha_vantage_key = api_keys.get('alpha_vantage')
    
    def fetch_company_news(self, symbol: str, lookback_hours: int = 24) -> List[Dict]:
        """
        Fetch recent news for a symbol from multiple sources
        """
        all_news = []
        
        # Source 1: NewsAPI.org (free tier: 100 requests/day)
        newsapi_articles = self._fetch_from_newsapi(symbol, lookback_hours)
        all_news.extend(newsapi_articles)
        
        # Source 2: Alpha Vantage News Sentiment
        av_articles = self._fetch_from_alpha_vantage(symbol)
        all_news.extend(av_articles)
        
        # Deduplicate by URL
        seen_urls = set()
        unique_news = []
        for article in all_news:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_news.append(article)
        
        return unique_news
    
    def _fetch_from_newsapi(self, symbol: str, lookback_hours: int) -> List[Dict]:
        """NewsAPI.org integration"""
        url = "https://newsapi.org/v2/everything"
        params = {
            'q': symbol,
            'from': (datetime.now() - timedelta(hours=lookback_hours)).isoformat(),
            'sortBy': 'publishedAt',
            'apiKey': self.news_api_key
        }
        
        response = requests.get(url, params=params)
        if response.status_code == 200:
            articles = response.json().get('articles', [])
            return [{
                'title': a['title'],
                'url': a['url'],
                'source': a['source']['name'],
                'published_at': a['publishedAt'],
                'description': a['description']
            } for a in articles]
        return []
    
    def _fetch_from_alpha_vantage(self, symbol: str) -> List[Dict]:
        """Alpha Vantage News Sentiment API"""
        url = f"https://www.alphavantage.co/query"
        params = {
            'function': 'NEWS_SENTIMENT',
            'tickers': symbol,
            'apikey': self.alpha_vantage_key
        }
        
        response = requests.get(url, params=params)
        if response.status_code == 200:
            feed = response.json().get('feed', [])
            return [{
                'title': item['title'],
                'url': item['url'],
                'source': item['source'],
                'published_at': item['time_published'],
                'sentiment_score': item['overall_sentiment_score'],
                'sentiment_label': item['overall_sentiment_label']
            } for item in feed]
        return []
Module 2: SEC Filings Scraper

python
# live_monitoring/narrative/sec_filings.py

import requests
from bs4 import BeautifulSoup

class SECFilingsScraper:
    def __init__(self):
        self.base_url = "https://www.sec.gov/cgi-bin/browse-edgar"
    
    def get_recent_filings(self, ticker: str, filing_types: List[str] = ['10-Q', '10-K', '8-K']) -> List[Dict]:
        """
        Fetch recent SEC filings for a ticker
        """
        params = {
            'action': 'getcompany',
            'CIK': ticker,
            'type': '',
            'dateb': '',
            'owner': 'exclude',
            'count': 100
        }
        
        response = requests.get(self.base_url, params=params, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(response.content, 'html.parser')
        
        filings = []
        for row in soup.find_all('tr')[1:]:  # Skip header
            cols = row.find_all('td')
            if len(cols) >= 4:
                filing_type = cols[0].text.strip()
                if filing_type in filing_types:
                    filings.append({
                        'type': filing_type,
                        'date': cols[3].text.strip(),
                        'description': cols[2].text.strip(),
                        'link': f"<https://www.sec.gov{cols>[1].find('a')['href']}"
                    })
        
        return filings[:10]  # Return 10 most recent
    
    def extract_risk_factors(self, filing_url: str) -> List[str]:
        """
        Extract 'Risk Factors' section from 10-Q/10-K
        (Simplified - full implementation needs better parsing)
        """
        response = requests.get(filing_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for "Risk Factors" section
        text = soup.get_text()
        risk_section_start = text.find("Risk Factors")
        if risk_section_start == -1:
            return []
        
        risk_section = text[risk_section_start:risk_section_start+5000]
        
        # Extract bullet points or paragraphs (simplified)
        risks = [line.strip() for line in risk_section.split('\n') if len(line.strip()) > 50]
        
        return risks[:20]  # Top 20 risk factors
Module 3: Earnings Transcript Fetcher

python
# live_monitoring/narrative/earnings_transcripts.py

import requests
from bs4 import BeautifulSoup

class EarningsTranscriptFetcher:
    def fetch_latest_transcript(self, symbol: str) -> Dict:
        """
        Fetch latest earnings call transcript from Seeking Alpha
        (Requires Seeking Alpha API or web scraping)
        """
        # Seeking Alpha transcripts URL pattern
        url = f"https://seekingalpha.com/symbol/{symbol}/earnings/transcripts"
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find latest transcript link (simplified)
        transcript_links = soup.find_all('a', href=True, text=lambda x: 'transcript' in x.lower() if x else False)
        
        if transcript_links:
            latest_link = transcript_links[0]['href']
            full_url = f"https://seekingalpha.com{latest_link}"
            
            # Fetch full transcript
            transcript_response = requests.get(full_url, headers=headers)
            transcript_soup = BeautifulSoup(transcript_response.content, 'html.parser')
            
            # Extract prepared remarks and Q&A
            transcript_text = transcript_soup.get_text()
            
            return {
                'url': full_url,
                'date': self._extract_date(transcript_soup),
                'management_remarks': self._extract_section(transcript_text, "Prepared Remarks"),
                'qa_section': self._extract_section(transcript_text, "Q&A")
            }
        
        return {}
    
    def _extract_section(self, text: str, section_name: str) -> str:
        """Extract specific section from transcript"""
        start = text.find(section_name)
        if start == -1:
            return ""
        end = text.find("Operator", start + len(section_name))
        return text[start:end] if end != -1 else text[start:start+5000]
PHASE 2: Divergence Detector (Week 2 - 8 hours)
python
# live_monitoring/narrative/divergence_detector.py

from typing import Dict, List
import re

class DivergenceDetector:
    def detect_narrative_gaps(self, symbol: str, 
                             official_narrative: Dict,
                             reality_data: Dict) -> List[Dict]:
        """
        Compare official narrative (earnings call, press releases)
        with reality data (SEC filings, dark pool, production numbers)
        """
        divergences = []
        
        # Example: Revenue guidance vs actual
        if 'revenue_guidance' in official_narrative and 'actual_revenue' in reality_data:
            guidance = official_narrative['revenue_guidance']
            actual = reality_data['actual_revenue']
            
            if abs(actual - guidance) / guidance > 0.05:  # >5% divergence
                divergences.append({
                    'type': 'REVENUE_MISS',
                    'severity': 'HIGH',
                    'official': f"Guided ${guidance}B",
                    'reality': f"Actual ${actual}B",
                    'gap': f"{((actual/guidance - 1) * 100):.1f}% {'beat' if actual > guidance else 'miss'}",
                    'signal': 'BEARISH' if actual < guidance else 'BULLISH'
                })
        
        # Example: Product claims vs production data
        if 'product_hype' in official_narrative and 'production_numbers' in reality_data:
            hype = official_narrative['product_hype']
            production = reality_data['production_numbers']
            
            # Check if hype mentions "record", "off the charts", etc.
            hype_keywords = ['record', 'off the charts', 'unprecedented', 'explosive']
            is_hyped = any(keyword in hype.lower() for keyword in hype_keywords)
            
            if is_hyped and production['growth_rate'] < 0.20:  # <20% growth
                divergences.append({
                    'type': 'PRODUCT_HYPE_MISMATCH',
                    'severity': 'MEDIUM',
                    'official': f"Management says '{hype}'",
                    'reality': f"Production growth only {production['growth_rate']*100:.1f}%",
                    'signal': 'BEARISH'
                })
        
        # Example: Customer concentration
        if 'customer_concentration' in reality_data:
            concentration = reality_data['customer_concentration']
            if concentration > 0.50:  # >50% from top 3 customers
                divergences.append({
                    'type': 'CUSTOMER_CONCENTRATION_RISK',
                    'severity': 'HIGH',
                    'official': 'Not disclosed in earnings call',
                    'reality': f"{concentration*100:.0f}% revenue from top 3 customers",
                    'signal': 'BEARISH'
                })
        
        return divergences
PHASE 3: Narrative Synthesizer (Week 3 - 6 hours)
python
# live_monitoring/narrative/narrative_synthesizer.py

class NarrativeSynthesizer:
    def generate_thesis(self, symbol: str, 
                       divergences: List[Dict],
                       institutional_data: Dict,
                       historical_patterns: List[Dict]) -> Dict:
        """
        Synthesize all narrative intelligence into actionable thesis
        """
        # Count bearish vs bullish signals
        bearish_count = sum(1 for d in divergences if d['signal'] == 'BEARISH')
        bullish_count = sum(1 for d in divergences if d['signal'] == 'BULLISH')
        
        # Weight by severity
        bearish_weight = sum(
            2 if d['severity'] == 'HIGH' else 1 
            for d in divergences if d['signal'] == 'BEARISH'
        )
        bullish_weight = sum(
            2 if d['severity'] == 'HIGH' else 1 
            for d in divergences if d['signal'] == 'BULLISH'
        )
        
        # Determine overall thesis
        if bearish_weight > bullish_weight * 1.5:
            thesis_direction = 'BEARISH'
            confidence = min(bearish_weight / (bearish_weight + bullish_weight) * 100, 95)
        elif bullish_weight > bearish_weight * 1.5:
            thesis_direction = 'BULLISH'
            confidence = min(bullish_weight / (bearish_weight + bullish_weight) * 100, 95)
        else:
            thesis_direction = 'NEUTRAL'
            confidence = 50
        
        # Generate human-readable summary
        summary = self._generate_summary(symbol, divergences, thesis_direction)
        
        # Generate trading implications
        trading_implications = self._generate_trading_implications(
            thesis_direction, 
            institutional_data,
            confidence
        )
        
        return {
            'symbol': symbol,
            'thesis_direction': thesis_direction,
            'confidence': confidence,
            'summary': summary,
            'key_divergences': divergences,
            'trading_implications': trading_implications,
            'historical_analogues': historical_patterns
        }
    
    def _generate_summary(self, symbol: str, divergences: List[Dict], direction: str) -> str:
        """Generate human-readable narrative summary"""
        summary_parts = [f"{symbol} Narrative Analysis:"]
        
        for div in divergences[:5]:  # Top 5 divergences
            summary_parts.append(
                f"â€¢ {div['type']}: {div['official']} vs {div['reality']}"
            )
        
        summary_parts.append(f"\nOverall Assessment: {direction}")
        
        return "\n".join(summary_parts)
    
    def _generate_trading_implications(self, direction: str, 
                                      institutional_data: Dict,
                                      confidence: float) -> Dict:
        """Convert narrative thesis into trading signals"""
        if direction == 'BEARISH' and confidence > 70:
            return {
                'action': 'SHORT',
                'entry': institutional_data.get('current_price'),
                'target': institutional_data.get('support_levels', [None])[0],
                'stop': institutional_data.get('resistance_levels', [None])[0],
                'rationale': 'Narrative divergences suggest downside, high confidence'
            }
        elif direction == 'BULLISH' and confidence > 70:
            return {
                'action': 'LONG',
                'entry': institutional_data.get('current_price'),
                'target': institutional_data.get('resistance_levels', [None])[0],
                'stop': institutional_data.get('support_levels', [None])[0],
                'rationale': 'Narrative supports upside, high confidence'
            }
        else:
            return {
                'action': 'WAIT',
                'rationale': f'Narrative inconclusive or confidence too low ({confidence:.1f}%)'
            }
ðŸŽ¯ INTEGRATION INTO SIGNAL GENERATOR
python
# live_monitoring/core/signal_generator.py (UPDATED)

from live_monitoring.narrative.narrative_engine import NarrativeEngine

class SignalGenerator:
    def __init__(self, use_lottery_mode=True, use_narrative_intelligence=True):
        self.use_narrative = use_narrative_intelligence
        if self.use_narrative:
            self.narrative_engine = NarrativeEngine(api_keys={
                'news_api': os.getenv('NEWS_API_KEY'),
                'alpha_vantage': os.getenv('ALPHA_VANTAGE_KEY')
            })
    
    def generate_signals(self, context: MarketContext) -> List[LiveSignal]:
        """Generate signals with narrative intelligence"""
        signals = []
        
        # Step 1: Generate technical signals (existing logic)
        technical_signals = self._generate_technical_signals(context)
        signals.extend(technical_signals)
        
        # Step 2: Enhance with narrative intelligence
        if self.use_narrative:
            narrative_thesis = self.narrative_engine.analyze(context.symbol)
            
            # Boost/reduce confidence based on narrative alignment
            for signal in signals:
                if narrative_thesis['thesis_direction'] == 'BEARISH' and signal.action == SignalAction.SELL:
                    signal.confidence *= 1.2  # Boost shorts when narrative bearish
                    signal.rationale += f" + NARRATIVE: {narrative_thesis['summary'][:100]}"
                elif narrative_thesis['thesis_direction'] == 'BULLISH' and signal.action == SignalAction.BUY:
                    signal.confidence *= 1.2  # Boost longs when narrative bullish
                    signal.rationale += f" + NARRATIVE: {narrative_thesis['summary'][:100]}"
                else:
                    signal.confidence *= 0.9  # Reduce if narrative conflicts
        
        return signals
ZETA'S IMPLEMENTATION ROADMAP:
Week 1: Multi-Source Aggregation (10 hours)

 News API integration (3 hours)

 SEC filings scraper (3 hours)

 Earnings transcript fetcher (2 hours)

 Test data collection (2 hours)

Week 2: Divergence Detection (8 hours)

 Build divergence detector (4 hours)

 Test on Nvidia earnings (2 hours)

 Tune detection thresholds (2 hours)

Week 3: Narrative Synthesis (6 hours)

 Build narrative synthesizer (3 hours)

 Integrate with signal generator (2 hours)

 End-to-end test (1 hour)

Total: 24 hours over 3 weeks = Narrative Intelligence complete